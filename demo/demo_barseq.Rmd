---
title: "HBMAP for BARseq"
author: "Huizi Zhang"
date: "2025-07-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  dpi=300,
  comment = "#>",
  fig.path = "man/figures/README-"
)
knitr::knit_hooks$set(imgcenter = function(before, options, envir){
  if (before) {
    HTML("<p align='center'>")
  } else {
    HTML("</p>")
  }
})
```

We load the data and necessary functions for MCMC sampling.

```{r package}
# load required packages
library(ggplot2)
library(ggrepel)
library(dplyr)
library(HBMAP)
```

```{r data, warning=FALSE}
data("data_barseq")

M <- length(data_barseq)
C <- sapply(1:M, function(m) ncol(data_barseq[[m]]))
R <- nrow(data_barseq[[1]])

regions.name <- rownames(data_barseq[[1]])

mouse.index <- c(rep(1, C[1]),
                 rep(2, C[2]),
                 rep(3, C[3]))
```

Set empirical parameters and initial clustering.

```{r empirical estiamtes, out.width='60%', fig.align='center'}
# ---------------- Empirical parameters --------
#Initialize the clustering and set hyperparameters empirically from the data
data_barseq_cbind <- do.call(cbind, data_barseq)

C_cumsum <- c(0, cumsum(sapply(1:M, function(m) ncol(data_barseq[[m]]))))

# initialize with k-means with cosine distance
df <- lapply(1:M,function(m){
  normalized_mat <- apply(data_barseq[[m]], 2, function(col) col / max(col))})

df <- t(do.call(cbind, df))

cosine_normalize <- function(mat) {
  # Normalize each row by its magnitude
  return(mat / sqrt(rowSums(mat^2)))
}

df <- cosine_normalize(mat = df)

wss = apply(matrix(seq(20,80,2),ncol=1),1,function(x){
  kmeans_result <- kmeans(df, centers = x, iter.max = 100, nstart = 50)
  kmeans_result$tot.withinss
})
ggplot() + geom_point(aes(x=seq(20,80,2),y=wss))

# Based on the plot, 40 seems like a reasonable number of clusters to start with
# initial clustering
kmeans_result <- kmeans(df, centers = 40, iter.max = 100, nstart = 50)$cluster
# Set the truncation to be larger to allow the model to explore more clusters
J = 70

clustinit <- lapply(1:M,
                    function(m) kmeans_result[(C_cumsum[m]+1):C_cumsum[m+1]])

# Empirical choice of alpha parameters 
a_alpha0  <- mean(unlist(lapply(clustinit,function(x){length(unique(x))}))/log(C))
a_alpha  <- length(unique(unlist(clustinit)))/log(sum(unlist(lapply(clustinit,function(x){length(unique(x))}))))

# Set the prior expectation of gamma to scale with the total counts
a_gamma = median(unlist(lapply(data_barseq,colSums)))/2
b_gamma = 1

tau = 0.5
nu = 1/1000
```


# Infer the clustering

```{r full algorithm}
# ---- parameters to pass to the main function ------
# mcmc setup
mcmc_list = list(number_iter = 40000, thinning = 5, burn_in = 20000, adaptive_prop = 0.0001,
                 auto_save = FALSE,
                 save_path = NULL,
                 save_frequency = 1000
                 )
# prior parameters, default values will be used if not provided
prior_list = list(a_gamma = a_gamma, b_gamma = b_gamma, lb_gamma = 5, a = 2, tau = tau, nu = nu,
                  a_alpha = a_alpha, b_alpha = 1, a_alpha0 = a_alpha0, b_alpha0 = 1)


# ------- Run the full model ---------
set.seed(3)
mcmc_all_barseq <- HBMAP_mcmc(Y = data_barseq, J = J, mcmc = mcmc_list, prior = prior_list, 
                            Z.init = NULL, verbose = TRUE)

```


## Traceplots and acceptance probability

```{r traceplots1, out.width='60%', fig.align='center'}
# ------- MCMC check -----------
ind <- seq(1,mcmc_all_barseq$output_index,by=1)

Zmat = matrix(unlist(mcmc_all_barseq$Z_output), length(mcmc_all_barseq$Z_output), sum(C),byrow = TRUE)

## ----- Number of occupied components -------
par(mfrow=c(1,1))
k = apply(Zmat,1,function(x){length(unique(x))})
plot(k, type = 'l')
```
```{r traceplots2, out.width='80%', fig.align='center'}
## ---- Concentration parameters ------
par(mfrow=c(1,2))
plot(mcmc_all_barseq$alpha_zero_output, type = 'l',ylab='',main='alpha0')
plot(mcmc_all_barseq$alpha_output, type = 'l',ylab='',main='alpha')
## ----- q, gamma and weights may have label switching, better check their convergence from the post-processing step below -------
# j=5
# r=4
# plot(unlist(lapply(mcmc_all_barseq$q_star_1_J_output[ind], function(q){q[j,r]})), type = 'l', ylab='', main=paste0('q[',j,',',r,']'))
# j=5
# plot(unlist(lapply(mcmc_all_barseq$gamma_star_1_J_output[ind], function(q){q[j]})), type = 'l', ylab='',main=paste0('gamma[',j,']'))
# j=29
# m=4
# plot(unlist(lapply(mcmc_all_barseq$omega_J_M_output[ind], function(q){q[j,m]})), type = 'l', ylab='', main=paste0('w[',j,',',m,']'))
# j=1
# plot(unlist(lapply(mcmc_all_barseq$omega_output[ind], function(q){q[j]})), type = 'l', ylab='', main=paste0('w[',j,']'))


## ------ Acceptance rate ------
par(mfrow=c(2,3))
plot(unlist(mcmc_all_barseq$acceptance_prob$q_star),type = 'l', ylab='', main='acceptance_q')
plot(unlist(mcmc_all_barseq$acceptance_prob$gamma_star), type = 'l', ylab='', main='acceptance_gamma')
plot(unlist(mcmc_all_barseq$acceptance_prob$alpha), type = 'l', ylab='', main='acceptance_alpha')
plot(unlist(mcmc_all_barseq$acceptance_prob$alpha_zero), type = 'l', ylab='', main='acceptance_alpha0')
plot(unlist(mcmc_all_barseq$acceptance_prob$omega), type = 'l', ylab='', main='acceptance_w')
```

## Optimal clustering

```{r optimal clustering, out.width='60%', fig.align='center', warning=FALSE}

# Posterior similarity matrix
psm_barseq = similarity_matrix(mcmc_run_all_output = mcmc_all_barseq)


# Reordered posterior samples of z
barseq_z_reordered <- z_trace_updated(mcmc_run_all_output = mcmc_all_barseq)


# optimal clustering
set.seed(1)
barseq_Z <- opt.clustering.comb(z_trace = barseq_z_reordered,
                                post_similarity = psm_barseq$psm.combined,
                                max.k = max(k))


#-- Convert to a list
C_cumsum <- c(0, cumsum(C))

barseq_Z <- lapply(1:M,
                   function(m) barseq_Z[(C_cumsum[m]+1):C_cumsum[m+1]])

```

Posterior similarity matrix can be used to quantify the uncertainty in clustering.

```{r psm, out.width='60%', fig.align='center'}
# Plot of posterior similarity matrix
psm_barseq_plot <- plotpsm(psm.ind = psm_barseq$psm.within,
                           psm.tot = psm_barseq$psm.combined)

psm_barseq_plot$plot.ind

```


# Post-processing step with a fixed clustering

```{r post-processing step}

mcmc_list = list(number_iter = 15000, thinning = 5, burn_in = 5000, adaptive_prop = 0.0001,
                 auto_save = FALSE,
                 save_path = NULL,
                 save_frequency = 1000
                 )
prior_list = list(a_gamma = a_gamma, b_gamma = b_gamma, lb_gamma = 5, a = 2, tau = tau, nu = nu,
                  a_alpha = a_alpha, b_alpha = 1, a_alpha0 = a_alpha0, b_alpha0 = 1)

# for the post-processing step, there is no label switching and we can make inference for each cluster based on q, gamma and weights
post_list = list(run_omega = TRUE, run_q_gamma = TRUE)

set.seed(3)
mcmc_barseq_post <- HBMAP_mcmc(Y = data_barseq, mcmc = mcmc_list, prior = prior_list, 
                               Z.fix = barseq_Z, post = post_list, verbose = TRUE)
```

We reorder the clusters based on estimated projection strengths.

```{r cluster reorder}
## ----- Reorder clusters based on estimated projection strengths ---------
### ----- return reordered samples as well for q, gamma and omegas ------
### ----- return summary statistics for q and gamma, and cluster labels (projected regions) ------
### ----- this is based on the probability of projections strength greater than a threshold (q_tilde) -------------
mcmc_barseq_post_reorder <- mcmc_reorder_cluster(post_output = mcmc_barseq_post, 
                                                 Z = barseq_Z, regions.name = rownames(data_barseq[[1]]))

# Neuron allocations after reordering
barseq_z_reordered <- mcmc_barseq_post_reorder$Z
```

Below we summarize cluster size, visualize neuron projection strengths within each cluster and estimated projection strength.

```{r cluster summary and line plots, message=FALSE, out.width='70%', fig.align='center'}
# you can choose your preferred color palette (change 'col' argument)
## ------ cluster size: number of neurons in each cluster, colored by group (mouse/injection site) ---------
opt.clustering.frequency(clustering = barseq_z_reordered, group.index = mouse.index, group.name = 'mouse', 
                         title = 'Cluster size')

## ---------- Heatmap of empirical projection strength of neurons in each cluster, colored by group (mouse/injection site) ----------
heatmap_ps(Y = data_barseq, Z = barseq_z_reordered, regions.name = rownames(data_barseq[[1]]), 
           group.index = mouse.index, group.name = 'mouse',
           cluster.index = 1:length(unique(unlist(barseq_z_reordered))), title = '')


## --------- Line plot for of empirical projection strengths within each cluster ------------------------
plot_empirical_ps(Y = data_barseq, Z = barseq_z_reordered, 
                  cluster.labels = mcmc_barseq_post_reorder$cluster.labels,
                  regions.name = rownames(data_barseq[[1]]),
                  group.index = mouse.index, group.name = 'mouse',
                  cluster.index = 1:length(unique(unlist(barseq_z_reordered))),
                  title = 'Empirical projection strength', facet_ncol = 6)

# ---------- Line plot of estimated projection strength within each cluster ----------
# get summary statistics
ps_summary <- ps_summarize(post_output_reorder = mcmc_barseq_post_reorder)
# plot
plot_estimated_ps(ps_summary = ps_summary, cluster.index = 1:length(unique(unlist(barseq_z_reordered))), 
                  title = 'Estimated projection strength', facet_ncol = 8)

```

## Prominent motifs
Below we find prominent motifs based on the posterior samples of the global weights.

```{r prominent motifs}
# ------- Find prominent motifs: Probability of the global weight greater than a threshold ------- 
# prominent motifs are defined as those with a high probability (> 0.95)
# below the function returns the indices of prominent motifs and the posterior probability of the global weight greater than thresh
prominent_motifs <- identify_prominent_motif(post_output_reorder = mcmc_barseq_post_reorder, 
                                             thresh = 0.007, prob = 0.95)
```

We could plot the estimated and empirical projection strengths for these prominent motifs only.

```{r plot prominent motifs, out.width='70%', fig.align='center'}
# redo the lines plots of empirical and estimated projection strengths for prominent motifs only
plot_empirical_ps(Y = data_barseq, Z = barseq_z_reordered, 
                  cluster.labels = mcmc_barseq_post_reorder$cluster.labels,
                  regions.name = rownames(data_barseq[[1]]),
                  group.index = mouse.index, group.name = 'mouse', 
                  cluster.index = prominent_motifs$index, 
                  title = 'Empirical projection strength (prominent motif)', 
                  facet_ncol = 4)
plot_estimated_ps(ps_summary = ps_summary, cluster.index = prominent_motifs$index, 
                  title = 'Estimated projection strength (prominent motif)', facet_ncol = 5)
```


In addition, we can investigate the posterior probabilities of allocation to these prominent motifs as well as the uncertainty from the posterior similarity matrix.

```{r allocation prob, out.width='60%',out.height='60%' ,fig.align='center'}
# line plots of empirical projection strengths colored by allocation probabilities
allo.prob = allocation_probability(post_output_reorder = mcmc_barseq_post_reorder, 
                                   Y = data_barseq)

projection_probability(Y = data_barseq, Z = barseq_z_reordered, 
                       cluster.labels = mcmc_barseq_post_reorder$cluster.labels,
                       regions.name = rownames(data_barseq[[1]]),
                       allocation_prob = allo.prob,
                       cluster.index = 1:length(unique(unlist(barseq_z_reordered))))
```

```{r superheat heatmap, out.width='60%',out.height='60%', fig.align='center'}
# plot the posterior similarity matrix for prominent motifs only
# to install the superheat package:
# install.packages("devtools")
# devtools::install_github("rlbarter/superheat")
library(superheat)
Z_factored = as.character(unlist(barseq_z_reordered))
ind = sapply(unlist(barseq_z_reordered), function(z){
  any(z == prominent_motifs$index)
})
superheat(psm_barseq$psm.combined[ind,ind],
          pretty.order.rows = TRUE,
          pretty.order.cols = TRUE,
          heat.pal = c("white", "yellow", "red"),
          heat.pal.values = c(0,.5,1),
          membership.rows = Z_factored[ind],
          membership.cols = Z_factored[ind],
          bottom.label.text.size = 4,
          left.label.text.size = 4)
```


## Variable motifs

We identify variable motifs by comparing the variance of the brain-specific weights across mice with a null model.

```{r variable motif}
mcmc_list = list(number_iter = 15000, thinning = 5, burn_in = 5000, adaptive_prop = 0.0001,
                 auto_save = FALSE,
                 save_path = NULL,
                 save_frequency = 1000
)
post_list <- list(run_omega=TRUE, run_q_gamma=FALSE)
# increase N to obtain more reliable results, e.g. N=200
# For windows users, use cluster_type='PSOCK',
set.seed(2)
local_weights_analysis_vc <- local_weights_analysis(N = 50, Z = mcmc_barseq_post_reorder$Z, 
                                                    omega_output = mcmc_barseq_post_reorder$omega_output,
                                                    omega_J_M_output = mcmc_barseq_post_reorder$omega_J_M_output, 
                                                    prior = prior_list, mcmc = mcmc_list, 
                                                    post = post_list, n_cores=4, 
                                                    cluster_type = 'FORK',
                                                    verbose = FALSE)

```

The results are summarized in the following figures.

```{r variable motif plot, out.width='60%', fig.align='center'}
# local weights variance
w_jm_empirical <- mcmc_barseq_post_reorder$omega_J_M_output
w_jm_variance_empirical <- lapply(1:length(w_jm_empirical),
                                  function(t) matrix(apply(w_jm_empirical[[t]], 1, var),
                                                     nrow = 1))
w_jm_variance_empirical <- do.call(rbind, w_jm_variance_empirical)
w_jm_variance_empirical <- colMeans(w_jm_variance_empirical)
local_weights_analysis_vc$variance_empirical <- w_jm_variance_empirical

local_weights_analysis_vc %>%
  ggplot(mapping = aes(x = variance_empirical, y = probability))+
  geom_point()+
  geom_text_repel(aes(label = cluster))+
  theme_bw()+
  xlab('variance of local weights')+
  ylab('probability of observing larger variance')+
  geom_hline(yintercept = 0.95)

# global weights
local_weights_analysis_vc$global_weight = apply(matrix(unlist(mcmc_barseq_post_reorder$omega_output), 
                                                       length(unique(unlist(barseq_z_reordered))), 
                                                       length(mcmc_barseq_post_reorder$omega_output)),1,mean)
local_weights_analysis_vc$probability_global = prominent_motifs$prob

local_weights_analysis_vc %>%
  ggplot(mapping = aes(x = global_weight, y = probability_global))+
  geom_point()+
  geom_text_repel(aes(label = cluster))+
  theme_bw()+
  xlab('mean of the global weight')+
  ylab('probability of global weight>0.02')+
  geom_hline(yintercept = 0.95) +
  geom_vline(xintercept = 0.02)
```

## Total variation distance between mice

We quantify differences between mice based on the total variation (TV) distance between brain-specific mixtures.

```{r TV distance, out.width='80%', fig.align='center'}
## ---- plot a heatmap for the posterior mean (and optionally labelled by credible intervals) of the tv_distance ----
plot_tv_distance(mcmc_run_all_output = mcmc_all_barseq, hpd = TRUE, prob = 0.95, text_size = 4)
```




## Summary

Overall we can summarize the estimated projection strength, global and local weights as well as posterior expected projection strength.

```{r summary plot, out.width='80%', fig.align='center'}
# ------ Summary plot ---------
# posterior expected number of counts/N for each region 
eps <- lapply(1:M, function(m) post_epstrength(m,mcmc_all_barseq))

# summarize the posterior mean of q, omega, omega_JM (reordered)
params_summ <- list(proj_prob = mcmc_barseq_post_reorder$proj_prob_mean, 
                    omega_JM = Reduce('+', mcmc_barseq_post_reorder$omega_J_M_output)/length(mcmc_barseq_post_reorder$omega_J_M_output),
                    omega = colMeans(do.call(rbind, mcmc_barseq_post_reorder$omega_output)))

plot_summary(params = params_summ, eps = eps, prominent_motifs_prob = prominent_motifs$prob, 
             prominent_motifs_thresh = 0.95, global_weight_thresh = 0.007,
             data.source.label = 'Mouse', regions.name = rownames(data_barseq[[1]]), 
             col_bar = c("deepskyblue","darkgrey","aquamarine"), col_mat = c('white','blue'), 
             legend = TRUE, legend_x = 0.6)
```


# Posterior predictive checks

For posterior predictive checks, we generate replicated data from the posterior and compare them with the true observed data.

```{r ppc, out.width='70%', fig.align='center'}
## --- a single replicate -----
# compare empirical projection strengths for each mouse
set.seed(3)
ppc_single_result <- ppc_single(mcmc_run_all_output = mcmc_all_barseq,
                                Y = data_barseq,
                                regions.name = rownames(data_barseq[[1]]))
m=3
print(ppc_single_result[[m]])

## ----- multiple replicate: compare number of zero counts for each region (barplot) ------
### ------ compare distribution of non-zero counts for each region (boxplot) -----------

ppc_multiple_result <- ppc_multiple(mcmc_run_all_output = mcmc_all_barseq,
                                   Y = data_barseq,
                                   N = 3,
                                   regions.name = rownames(data_barseq[[1]]))
ppc_multiple_result$zero.plot
ppc_multiple_result$non.zero.plot
```

